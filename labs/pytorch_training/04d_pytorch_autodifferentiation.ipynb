{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pleased-allen",
   "metadata": {},
   "source": [
    "## PyTorch Autograd functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "centered-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "unlikely-swaziland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "torch.get_default_dtype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "pleasant-musician",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.tensor([[1, 2, 3],\n",
    "                         [4, 5, 6]], dtype=torch.float32)\n",
    "tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "eastern-amount",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 20., 30.],\n",
       "        [40., 50., 60.]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2 = torch.tensor([[10, 20, 30],\n",
    "                        [40, 50, 60]], dtype=torch.float32)\n",
    "tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "dietary-williams",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "victorian-bathroom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-coating",
   "metadata": {},
   "source": [
    "By default, every PyTorch Tensor has a requires_grad property. When True, Pytorch tracks tensor computations in the forward pass and calculates gradients in the backward pass.  \n",
    "\n",
    "If not set to True, to enable tracking history that allows calculation of gradients with respect to the Tensor, _call_ the `requires_grad_()` function which sets the value to True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "fatty-scottish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]], requires_grad=True)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1.requires_grad_() # Updates the requires grad property IN-Place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "blind-level",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1.requires_grad # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "single-payday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2.requires_grad # still False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-basics",
   "metadata": {},
   "source": [
    "Tensors and functions make up our **DAG**, *directed acyclic computation graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "appreciated-spokesman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6667,  3.3333,  5.0000],\n",
      "        [ 6.6667,  8.3333, 10.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor1.grad) # no gradients have been calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "attended-warehouse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tensor1.grad_fn) # the computational tree is empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-microphone",
   "metadata": {},
   "source": [
    "Tensors are the nodes in this graph and the functions are the transformations along the edges. Together tensors & functions make up the Directed Acyclic Computation Graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-technique",
   "metadata": {},
   "source": [
    "Every tensor has a grad function to create that function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-thomas",
   "metadata": {},
   "source": [
    "Let's set up our first Simple computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "measured-marijuana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 10.,  40.,  90.],\n",
       "        [160., 250., 360.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor = tensor1 * tensor2\n",
    "output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-realtor",
   "metadata": {},
   "source": [
    "The `requires_grad` property is based on the input tensors used to create it. `tensor1` requires_grad property was set to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "diagnostic-wales",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor.requires_grad "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-suite",
   "metadata": {},
   "source": [
    "The `grad` property of the `output_tensor` is `None` because there are no gradients created from a backward pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "orange-aircraft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-108-2e7da3899fae>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  print(output_tensor.grad)\n"
     ]
    }
   ],
   "source": [
    "print(output_tensor.grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "hawaiian-advertiser",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MulBackward0 object at 0x7f890a20bac0>\n"
     ]
    }
   ],
   "source": [
    "print(output_tensor.grad_fn) # multiplication based tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "first-emphasis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tensor1.grad_fn)\n",
    "print(tensor2.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-manner",
   "metadata": {},
   "source": [
    "Different operation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "painful-phone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MeanBackward0 object at 0x7f8908e6dee0>\n"
     ]
    }
   ],
   "source": [
    "output_tensor = (tensor1 * tensor2).mean()\n",
    "print(output_tensor.grad_fn) # grad_fn references the last function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "alert-proposal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6667,  3.3333,  5.0000],\n",
      "        [ 6.6667,  8.3333, 10.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor1.grad) # only when backward function is called on an output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "electronic-northwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor.backward() # partial derivatives now calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "norman-segment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.3333,  6.6667, 10.0000],\n",
      "        [13.3333, 16.6667, 20.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-serve",
   "metadata": {},
   "source": [
    "Gradients are partial derivatives wrt every value of tensor1, the shapes are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "declared-permission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.Size([2, 3]))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1.grad.shape, tensor1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "suitable-behavior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tensor2.grad) # none calculated, required_grad was False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "continental-times",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-117-95e125c76021>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  print(output_tensor.grad) # this is the value wrt the partial derivatives are calculated so it doesn't have gradients\n"
     ]
    }
   ],
   "source": [
    "print(output_tensor.grad) # this is the value wrt the partial derivatives are calculated so it doesn't have gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-medium",
   "metadata": {},
   "source": [
    "requires_grad property is propagated from inp tensors to res tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "thrown-excuse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "new_tensor = tensor1 * 3\n",
    "print(new_tensor.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-microwave",
   "metadata": {},
   "source": [
    "Grad func is mulbackward, mult op that created this tensor\n",
    "if tracking history is enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fixed-twins",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new tensor =  tensor([[ 3.,  6.,  9.],\n",
      "        [12., 15., 18.]])\n",
      "requires_grad for tensor =  True\n",
      "requires_grad for tensor1 =  False\n",
      "requires_grad for new_tensor =  False\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # to stop autograd to stop computations with tracking hist off\n",
    "    new_tensor = tensor1 * 3\n",
    "    print('new tensor = ', new_tensor)\n",
    "    print('requires_grad for tensor = ', tensor1.requires_grad)\n",
    "    print('requires_grad for tensor1 = ', tensor2.requires_grad)\n",
    "    print('requires_grad for new_tensor = ', new_tensor.requires_grad)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-governor",
   "metadata": {},
   "source": [
    "The original tensor1 has requires_grad = True and the new_tensor has it set to False, no propagation becase requires_grad was set to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "comfortable-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(t):\n",
    "    return t * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "governing-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def calculate_with_no_grad(t):\n",
    "    return t * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "decent-relay",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  4.,  6.],\n",
       "        [ 8., 10., 12.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_tensor = calculate(tensor1)\n",
    "result_tensor # grad function is MulBackward0 because it was created using a multiplication operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "entitled-fantasy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_tensor.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "public-booking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  4.,  6.],\n",
       "        [ 8., 10., 12.]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calc teh result tensor but with no_grad decorator\n",
    "# @torch.no_grad()\n",
    "result_tensor_no_grad = calculate_with_no_grad(tensor1)\n",
    "result_tensor_no_grad # does not keep track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "protective-western",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_tensor_no_grad.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-wallpaper",
   "metadata": {},
   "source": [
    "**Nesting Inner blocks where you want history tracking and gradients to be calculated within an outer no_grad block**: We can enable history tracking of gradients within a no_grad block: In general we want to turn it off but we can turn it off for select computations within a `no_grad` block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "magnetic-garden",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_tensor_no_grad =  tensor([[ 3.,  6.,  9.],\n",
      "        [12., 15., 18.]])\n",
      "new_tensor_grad =  tensor([[ 3.,  6.,  9.],\n",
      "        [12., 15., 18.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    new_tensor_no_grad = tensor1 * 3\n",
    "    print('new_tensor_no_grad = ', new_tensor_no_grad)\n",
    "    \n",
    "    with torch.enable_grad():\n",
    "        new_tensor_grad = tensor1 * 3\n",
    "        print('new_tensor_grad = ', new_tensor_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-senior",
   "metadata": {},
   "source": [
    "If ANY of the input tensors have their `requires_grad` property set to True, then the computed tensors will as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "assigned-wagon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], requires_grad=True)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_one = torch.tensor([[1.0, 2.0],\n",
    "                           [3.0, 4.0]], requires_grad=True)\n",
    "tensor_one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-feeding",
   "metadata": {},
   "source": [
    "Create another tensor, by default requires_grad will be set to False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "vocational-mailman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 6.],\n",
       "        [7., 8.]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_two = torch.Tensor([[5, 6],\n",
    "                           [7, 8]])\n",
    "tensor_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "killing-nebraska",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_one.requires_grad, tensor_two.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "hearing-functionality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 6.],\n",
       "        [7., 8.]], requires_grad=True)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_two.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "relevant-pollution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_one.requires_grad, tensor_two.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "nearby-lexington",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tensor = (tensor_one + tensor_two).mean()\n",
    "final_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-estimate",
   "metadata": {},
   "source": [
    "So far, we have only completed forward pass; there are no gradients associated with `tensor_one`, nor with `tensor_two`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "considered-event",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tensor.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "integrated-annex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2500, 0.2500],\n",
      "        [0.2500, 0.2500]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor_one.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "equipped-organizer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2500, 0.2500],\n",
      "        [0.2500, 0.2500]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor_two.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-europe",
   "metadata": {},
   "source": [
    "Tensors involved in a computation are part of a larger computation graph. If you want to detach a tensor from the graph, you can call the detach function on tensor. **The detached tensor will always have its `requires_grad` property set to `False`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "suffering-musical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detached_tensor = tensor_one.detach()\n",
    "detached_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "regulation-coordinator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use these in a computation\n",
    "mean_tensor = (tensor_one + detached_tensor).mean()\n",
    "mean_tensor.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "proper-isolation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000],\n",
       "        [0.5000, 0.5000]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_one.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ahead-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tensor.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "varying-saver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7500, 0.7500],\n",
       "        [0.7500, 0.7500]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_one.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "nasty-strength",
   "metadata": {},
   "outputs": [],
   "source": [
    "detached_tensor.grad # none calculated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-offense",
   "metadata": {},
   "source": [
    "### Variable API\n",
    "\n",
    "** The Variable API in PyTorch has been deprecated**: Variables are no longer needed to work with autograd and to store gradients. \n",
    "\n",
    "Variables were tailored to hold values during training. The gradient vector, grad fnction etc are now part of the Tensor function itself. \n",
    "\n",
    "You can still use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "reverse-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "later-buyer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([9.]), False)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = Variable(torch.FloatTensor([9]))\n",
    "var, var.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "developed-catholic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.], requires_grad=True)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "honest-water",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.], requires_grad=True)\n",
      "tensor([10.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w1 = Variable(torch.FloatTensor([3]), requires_grad=True)\n",
    "w2 = Variable(torch.FloatTensor([10]), requires_grad=True)\n",
    "print(w1)\n",
    "print(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "wrapped-yellow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([27.], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_var = var * w1\n",
    "result_var # this is also a `tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "adolescent-winning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_var.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-solomon",
   "metadata": {},
   "source": [
    "The `backward()` function call will work exactly the same way: it calculates the gradients for all of the input tensors where `requires_grad` property was set to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "wrapped-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_var.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-humanitarian",
   "metadata": {},
   "source": [
    "* `w1.grad` contains the gradients for `w1` with respect to `result_var`\n",
    "* `w2` was not involved in the operation so it will not contain any gradients\n",
    "* and `var` will contain gradients as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "transsexual-edgar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([9.]), None, tensor([3.]))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(w1.grad, w2.grad, var.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "automatic-clear",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-latvia",
   "metadata": {},
   "source": [
    "# Building a linear model with Autograd\n",
    "\n",
    "**See**\n",
    "[04e_demo7-LinearModelUsingAutograd_ys_notes](04e_demo7-LinearModelUsingAutograd_ys_notes.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-certification",
   "metadata": {},
   "source": [
    "# Dynamic Computation\n",
    "\n",
    "**See**\n",
    "[04f_dynamic_computation](04f_dynamic_computation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-asian",
   "metadata": {},
   "source": [
    "## Backpropagation \n",
    "\n",
    "Building graphs dyanamically using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "chubby-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "from graphviz import Source\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "impressive-radical",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No image data found. Expecting filename, url, or data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-177-d67179c4ba10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nn3.gv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_ext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No image data found. Expecting filename, url, or data.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         elif isinstance(data, str) and (\n\u001b[1;32m   1182\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_safe_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No image data found. Expecting filename, url, or data."
     ]
    }
   ],
   "source": [
    "temp = \"\"\"\n",
    "digraph G{\n",
    "edge [dir=forward]\n",
    "node [shape=square]\n",
    "x [label=\"x\"]\n",
    "w_x [label=\"w_x\"]\n",
    "x -> x_prod\n",
    "x_prod [label=\"*\\nx_prod\"]\n",
    "w_x -> x_prod\n",
    "\n",
    "h [label=\"h\"]\n",
    "w_h [label=\"w_h\"]\n",
    "h -> h_prod\n",
    "w_h -> h_prod\n",
    "h_prod [label=\"*\\nh_prod\"]\n",
    "x_prod -> z\n",
    "h_prod -> z\n",
    "z [label=\"+\\nz\"]\n",
    "}\n",
    "\"\"\"\n",
    "s = Source(temp, filename=\"nn3.gv\", format=\"png\")\n",
    "Image('nn3.gv.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "extraordinary-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 10, requires_grad=True)\n",
    "prev_h = torch.rand(1, 20)\n",
    "W_h = torch.rand(20, 20)\n",
    "W_x = torch.rand(20, 10)\n",
    "\n",
    "i2h = torch.mm(W_x, x.t())\n",
    "h2h = torch.mm(W_h, prev_h.t())\n",
    "next_h = i2h + h2h\n",
    "next_h = next_h.tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "beginning-champion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "atlantic-driving",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_h.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "frozen-ecology",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-200-190f391c0d24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext_h\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "next_h.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "simple-profession",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0010925925925925927"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.95/2700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-underwear",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
